{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Autograd\n",
    "## 1.1 梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "autograd类的原理其实就是一个雅克比矩阵向量积计算引擎.   \n",
    "在张量间的计算过程中，如果在所有输入中，有一个输入需要求导，那么输出一定会需要求导；相反，只有当所有输入都不需要求导的时候，输出才会不需要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.) None None\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]]) None <MulBackward0 object at 0x000001FA91236888>\n",
      "tensor(40.) None <MeanBackward0 object at 0x000001FA95C020C8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\16383\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\build\\aten\\src\\ATen/core/TensorBody.h:482.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones([2, 2], requires_grad=False)    # 输入\n",
    "w1 = torch.tensor(2.0, requires_grad=True)     # 权重\n",
    "w2 = torch.tensor(3.0, requires_grad=True)\n",
    "w3 = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# forward()\n",
    "l1 = x * w1\n",
    "l2 = l1 + w2\n",
    "# l2.retain_grad() 保留非叶子节点的梯度\n",
    "# l1.register_hook(lambda grad: print('l1 grad: ', grad)) 只打印导数信息，不保存\n",
    "# 思考： pytorch中的钩子（Hook）有何作用？\n",
    "l3 = l1 * w3\n",
    "l4 = l2 * l3\n",
    "loss = l4.mean()\n",
    "\n",
    "\n",
    "print(w1.data, w1.grad, w1.grad_fn)\n",
    "\n",
    "\n",
    "print(l1.data, l1.grad, l1.grad_fn)\n",
    "\n",
    "\n",
    "print(loss.data, loss.grad, loss.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "正在访问 非叶子张量的 .grad 属性。 在 autograd.backward() 期间不会填充其 .grad 属性。    \n",
    "如果您确实希望为非叶子张量的 .grad 字段，请在非叶子张量上使用 .retain_grad() 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28.) tensor(8.) tensor(10.)\n",
      "None None None None None\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "\n",
    "print(w1.grad, w2.grad, w3.grad)\n",
    "\n",
    "print(l1.grad, l2.grad, l3.grad, l4.grad, loss.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 叶子张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在调用backward()时,只有当requires_grad和is_leaf同时为真时,才会计算节点的梯度值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones([2, 2], requires_grad=True)\n",
    "print(a.is_leaf)\n",
    "\n",
    "b = a + 2\n",
    "print(b.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "对于 requires_grad=False 的 tensor 来说，我们约定俗成地把它们归为叶子张量。\n",
    "\n",
    "当 requires_grad=True 的时候，如何判断是否是叶子张量：当这个 tensor 是用户创建的时候，它是一个叶子节点，当这个 tensor 是由其他运算操作产生的时候，它就不是一个叶子节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "思考：为什么需要叶子节点？     \n",
    "那些非叶子结点，是通过用户所定义的叶子节点的一系列运算生成的，也就是这些非叶子节点都是中间变量，一般情况下，用户不会去使用这些中间变量的导数，所以为了节省内存，它们在用完之后就被释放了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.3 in-place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "inplace 指的是在不更改变量的内存地址的情况下，直接修改变量的值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2336500244368\n",
      "0\n",
      "-----\n",
      "2336500417376\n",
      "0\n",
      "*****\n",
      "2337898266432\n",
      "0\n",
      "-----\n",
      "2337898266432\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 情景 1\n",
    "a = torch.tensor([3.0, 1.0])\n",
    "print(id(a)) \n",
    "print(a._version)\n",
    "print('-----')\n",
    "a = a.exp()\n",
    "print(id(a))\n",
    "print(a._version)\n",
    "\n",
    "\n",
    "print('*****')\n",
    "\n",
    "\n",
    "# 情景 2\n",
    "a = torch.tensor([3.0, 5.0, 7.0])\n",
    "print(id(a)) \n",
    "print(a._version)\n",
    "print('-----')\n",
    "a[0] = 10\n",
    "print(id(a)) \n",
    "print(a._version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2]], which is output 0 of struct torch::autograd::CopySlices, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32me:\\DeepLearning\\DataScience\\Pytorch教程\\课件\\Code\\Autograph.ipynb Cell 14\u001B[0m in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m b[\u001B[39m0\u001B[39m] \u001B[39m=\u001B[39m \u001B[39m1000.0\u001B[39m\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001B[0m \u001B[39mprint\u001B[39m(b\u001B[39m.\u001B[39m_version) \u001B[39m# 1\u001B[39;00m\n\u001B[1;32m---> <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001B[0m loss\u001B[39m.\u001B[39;49mbackward()\n",
      "File \u001B[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    387\u001B[0m \u001B[39mif\u001B[39;00m has_torch_function_unary(\u001B[39mself\u001B[39m):\n\u001B[0;32m    388\u001B[0m     \u001B[39mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    389\u001B[0m         Tensor\u001B[39m.\u001B[39mbackward,\n\u001B[0;32m    390\u001B[0m         (\u001B[39mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    394\u001B[0m         create_graph\u001B[39m=\u001B[39mcreate_graph,\n\u001B[0;32m    395\u001B[0m         inputs\u001B[39m=\u001B[39minputs)\n\u001B[1;32m--> 396\u001B[0m torch\u001B[39m.\u001B[39;49mautograd\u001B[39m.\u001B[39;49mbackward(\u001B[39mself\u001B[39;49m, gradient, retain_graph, create_graph, inputs\u001B[39m=\u001B[39;49minputs)\n",
      "File \u001B[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    168\u001B[0m     retain_graph \u001B[39m=\u001B[39m create_graph\n\u001B[0;32m    170\u001B[0m \u001B[39m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[39m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    172\u001B[0m \u001B[39m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 173\u001B[0m Variable\u001B[39m.\u001B[39;49m_execution_engine\u001B[39m.\u001B[39;49mrun_backward(  \u001B[39m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001B[0;32m    175\u001B[0m     allow_unreachable\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m, accumulate_grad\u001B[39m=\u001B[39;49m\u001B[39mTrue\u001B[39;49;00m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2]], which is output 0 of struct torch::autograd::CopySlices, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# 错误举例：发生in-place操作\n",
    "a = torch.tensor([1.0, 3.0], requires_grad=True)\n",
    "b = a + 2\n",
    "print(b._version) # 0\n",
    "\n",
    "loss = (b * b).mean()\n",
    "b[0] = 1000.0\n",
    "print(b._version) # 1\n",
    "\n",
    "loss.backward()\n",
    "# 在正向传播过程中，求导系统记录的 b 的 version 是0，\n",
    "# 但是在进行反向传播的过程中，求导系统发现 b 的 version 变成1了，所以就会报错了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  5.,  2.,  3.], requires_grad=True) True\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32me:\\DeepLearning\\DataScience\\Pytorch教程\\课件\\Code\\Autograph.ipynb Cell 15\u001B[0m in \u001B[0;36m<cell line: 6>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001B[0m a \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39mtensor([\u001B[39m10.\u001B[39m, \u001B[39m5.\u001B[39m, \u001B[39m2.\u001B[39m, \u001B[39m3.\u001B[39m], requires_grad\u001B[39m=\u001B[39m\u001B[39mTrue\u001B[39;00m)\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001B[0m \u001B[39mprint\u001B[39m(a, a\u001B[39m.\u001B[39mis_leaf)\n\u001B[1;32m----> <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001B[0m a[:] \u001B[39m=\u001B[39m \u001B[39m0\u001B[39m\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001B[0m \u001B[39mprint\u001B[39m(a, a\u001B[39m.\u001B[39mis_leaf)\n\u001B[0;32m      <a href='vscode-notebook-cell:/e%3A/DeepLearning/DataScience/Pytorch%E6%95%99%E7%A8%8B/%E8%AF%BE%E4%BB%B6/Code/Autograph.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001B[0m loss \u001B[39m=\u001B[39m (a\u001B[39m*\u001B[39ma)\u001B[39m.\u001B[39mmean()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# 错误举例：叶子节点变成了非叶子节点\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf)\n",
    "\n",
    "a[:] = 0\n",
    "print(a, a.is_leaf)\n",
    "\n",
    "loss = (a*a).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  5.,  2.,  3.], requires_grad=True) True 2175670744712\n",
      "tensor([5.0000, 2.5000, 1.0000, 1.5000])\n",
      "tensor([10., 10., 10., 10.], requires_grad=True) True 2175670744712\n",
      "tensor([10.0000,  7.5000,  6.0000,  6.5000])\n",
      "None\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 正确实例\n",
    "# 方法一\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf, id(a))\n",
    "loss = (a*a).mean()\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "\n",
    "# a.data.fill_(10.)\n",
    "a.detach().fill_(10.)\n",
    "print(a, a.is_leaf, id(a))\n",
    "\n",
    "\n",
    "loss = (a*a).mean()\n",
    "loss.backward()\n",
    "print(a.grad)\n",
    "print(a.grad_fn)\n",
    "print(a.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "tensor.detach() 返回一个新的tensor，从当前计算图中分离下来。但是仍指向原变量的存放位置，不同之处只是requirse_grad为false。新得到的这个tensor永远不需要计算梯度，不具有grad。\n",
    "\n",
    "即使之后重新将它的requires_grad置为true,它也不会具有梯度grad。这样我们就会继续使用这个新的tensor进行计算，后面当我们进行反向传播时，到该调用detach()的tensor就会停止，不能再继续向前进行传播。\n",
    "\n",
    "注意：使用detach返回的tensor和原始的tensor共同一个内存，即一个修改另一个也会跟着改变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.,  5.,  2.,  3.], requires_grad=True) True\n",
      "tensor([10., 10., 10., 10.], requires_grad=True) True\n",
      "tensor([5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# 方法二\n",
    "a = torch.tensor([10., 5., 2., 3.], requires_grad=True)\n",
    "print(a, a.is_leaf)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    a[:] = 10.\n",
    "print(a, a.is_leaf)\n",
    "\n",
    "\n",
    "loss = (a*a).mean()\n",
    "loss.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cfc530bfbf58ef669fe3ee46369fadc44820cf6ba210c674b67659f40614153f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}