{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特性\n",
    "1. 高维数组（GPU加速计算）   \n",
    "https://pytorch.org/docs/stable/tensors.html#torch.Tensor    \n",
    "https://zhuanlan.zhihu.com/p/511023132    \n",
    "https://zhuanlan.zhihu.com/p/497073752     \n",
    "2. 自动求导    \n",
    "类中有很多@property注解表示的方法，比如张量梯度计算函数的grad_fn....这些类方法可以当做属性被调用，如：tensor.device；而无注解的正常方法是通过方法调用，如tensor.tolist()。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtype   该张量存储的值类型，可选类型见：torch.dtype   \n",
    "device  该张量存放的设备类型，cpu/gpu    \n",
    "data    该张量节点存储的值   \n",
    "requires_grad  表示autograd时是否需要计算此tensor的梯度，默认False    \n",
    "grad    存储梯度的值，初始为None    \n",
    "grad_fn 反向传播时，用来计算梯度的函数   \n",
    "is_leaf 该张量节点在计算图中是否为叶子节点    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor类变量，布尔值，表示autograd时是否需要计算此tensor的梯度，默认False；    \n",
    "用官方文档上的话描述：requires_grad允许从梯度计算中细粒度地排除子图，并可以提高计算效率。    \n",
    "这里需要注意一点，某个操作/tensor构成的模型中，只要有单个输入需要记录梯度(requires_grad=True)，则该操作/模型的输出也必然需要记录梯度(否则梯度是无法传递到该tensor上的)。    \n",
    "当且仅当某个操作/模型上所有的输入都无需记录梯度时，输出才可以不记录梯度，设置为requires_grad=False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tensor创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = np.array([1, 2, 3])\n",
    "torch_tensor1 = torch.from_numpy(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor2 = torch.Tensor(numpy_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tnesor3 = torch.tensor(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 cpu()|gpu() 上计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch_tnesor3.is_cuda)\n",
    "torch_tensor4 = torch_tnesor3.cuda(0)     # .to(device='cuda:0')\n",
    "# torch_tensor= torch.zeros([2,3],dtype=torch.float64,device=torch.device('cuda:0'))\n",
    "print(torch_tensor4.is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将tensor投射为double类型：newtensor = tensor.double()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensor常用属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor1 = torch.ones([2,3])\n",
    "tensor1.dtype \n",
    "tensor1.shape\n",
    "tensor1.ndim\n",
    "tensor1.is_cuda\n",
    "tensor1.device\n",
    "tensor1.cuda(0)\n",
    "tensor1.device\n",
    "tensor1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tensor常用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.ones([2,1,3])\n",
    "torch_tensor1.size() #torch.Size([2, 1, 3])\n",
    "tensor2=torch.squeeze(tensor1)\n",
    "print(tensor2.size())#torch.Size([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n",
      "torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor1 = torch.ones([2,1,3])\n",
    "print(tensor1.size()) # torch.Size([2, 1, 3])\n",
    "tensor2 = tensor1.permute(2,1,0) # 0,1,2-> 2,1,0\n",
    "print(tensor2.size()) # torch.Size([3, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch.cat() \n",
    "# Torch.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. tensor.grad\n",
    "This attribute is None by default and becomes a Tensor the first time a call to\n",
    "backward() computes gradients for self. The attribute will then contain the\n",
    "gradients computed and future calls to backward() will accumulate (add)\n",
    "gradients into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/weixin_42782150/article/details/106116082   \n",
    "https://zhuanlan.zhihu.com/p/145353262\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68e895160ca3547caeae7a17bc740e4f922c86cb697ea67c628f6ca424cae509"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
